{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkConf, SparkContext, HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "executionDate = input(\"Input date you want transform data from HDFS DataLake and save to Hive Storage: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-09-13'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executionDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTime = executionDate.split(\"-\")\n",
    "year = runTime[0]\n",
    "month = runTime[1]\n",
    "day = runTime[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "23/08/01 21:37:57 WARN Utils: Your hostname, bigdata-etl resolves to a loopback address: 127.0.1.1; using 192.168.85.128 instead (on interface ens33)\n",
      "23/08/01 21:37:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/01 21:38:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/01 21:38:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# create spark session\n",
    "spark = SparkSession \\\n",
    "   .builder \\\n",
    "   .appName(\"Daily Gross Revenue Report\") \\\n",
    "   .config('hive.exec.dynamic.partition', 'true') \\\n",
    "   .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "   .config('spark.sql.warehouse.dir', 'hdfs://localhost:9000/user/hive/warehouse') \\\n",
    "   .enableHiveSupport() \\\n",
    "   .getOrCreate()\n",
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load data to spark df\n",
    "orders_df = spark.read.parquet('hdfs://localhost:9000/datalake/orders').drop(\"year\", \"month\", \"day\")\n",
    "order_detail_df = spark.read.parquet('hdfs://localhost:9000/datalake/order_detail').drop(\"year\", \"month\", \"day\")\n",
    "products_df = spark.read.parquet('hdfs://localhost:9000/datalake/products').drop(\"year\", \"month\", \"day\", \"created_at\")\n",
    "inventory_df = spark.read.parquet('hdfs://localhost:9000/datalake/inventory').drop(\"year\", \"month\", \"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+----------+\n",
      "| id|quantity|created_at|product_id|\n",
      "+---+--------+----------+----------+\n",
      "|  1|       1|2009-01-25|    331449|\n",
      "|  2|       1|2019-09-13|    182256|\n",
      "|  3|       2|2004-05-04|    108399|\n",
      "|  4|       3|2011-02-20|     81461|\n",
      "|  5|       3|2007-07-11|    136274|\n",
      "+---+--------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------+-------+\n",
      "| id| total|    payment|order_id|user_id|\n",
      "+---+------+-----------+--------+-------+\n",
      "|  1|710051|credit_card|       1| 209279|\n",
      "|  2|375643|       cash|       2| 242546|\n",
      "|  3|975362|       cash|       3| 135215|\n",
      "|  4|417644|credit_card|       4| 111433|\n",
      "|  5|481473|credit_card|       5|  44346|\n",
      "+---+------+-----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "order_detail_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+------------+------------+\n",
      "| id|    make|         model|    category|inventory_id|\n",
      "+---+--------+--------------+------------+------------+\n",
      "|  1|     BMW|      5 Series|Sedan, Wagon|      999830|\n",
      "|  2| Mercury| Grand Marquis|       Sedan|      988335|\n",
      "|  3|   Honda|          CR-V|         SUV|      986788|\n",
      "|  4|Cadillac|           XT5|         SUV|      986910|\n",
      "|  5|  Nissan|Titan Crew Cab|      Pickup|      988637|\n",
      "+---+--------+--------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|quantity|\n",
      "+---+--------+\n",
      "|  1|     355|\n",
      "|  2|     492|\n",
      "|  3|     269|\n",
      "|  4|     394|\n",
      "|  5|     239|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inventory_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = orders_df \\\n",
    "    .filter(orders_df[\"created_at\"] == \"2019-09-13\") \\\n",
    "    .join(order_detail_df, orders_df[\"id\"] == order_detail_df[\"order_id\"], \"inner\") \\\n",
    "    .join(products_df, orders_df[\"product_id\"] == products_df[\"id\"], \"inner\") \\\n",
    "    .join(inventory_df.select(f.col(\"quantity\").alias(\"inv_quantity\"), f.col(\"id\")), products_df[\"inventory_id\"] == inventory_df[\"id\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=========>         (1 + 1) / 2][Stage 11:>                 (0 + 1) / 2]\r"
     ]
    }
   ],
   "source": [
    "pre_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df = pre_df.groupBy(\"Make\", \"Model\", \"Category\", \"product_id\", \"inv_quantity\") \\\n",
    "    .agg(\n",
    "        f.sum(\"quantity\").alias(\"Sales\"),\n",
    "        f.sum(\"total\").alias(\"Revenue\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = map_df \\\n",
    "    .withColumn(\"LetfOver\", f.col(\"inv_quantity\") - f.col(\"Sales\")) \\\n",
    "    .withColumn(\"year\", f.lit(year)) \\\n",
    "    .withColumn(\"month\", f.lit(month)) \\\n",
    "    .withColumn(\"day\", f.lit(day)) \\\n",
    "    .select(\"Make\", \"Model\", \"Category\", \"Sales\", \"Revenue\", \"year\", \"month\", \"day\", \"LetfOver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|    haha1|\n",
      "|  reports|\n",
      "|     test|\n",
      "|    test1|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"show databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(info_value='spark_catalog'),\n",
       " Row(info_value='test'),\n",
       " Row(info_value=''),\n",
       " Row(info_value='file:/home/hadoop/Desktop/cars-sale-etl/etl_souce/spark-warehouse/test.db'),\n",
       " Row(info_value='hadoop')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.sql(\"describe database test\")\n",
    "df.select('info_value').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.write \\\n",
    "    .format(\"hive\") \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"reports.daily_gross_revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
