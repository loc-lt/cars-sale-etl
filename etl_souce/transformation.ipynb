{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkConf, SparkContext, HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "executionDate = input(\"Input date you want transform data from HDFS DataLake and save to Hive Storage: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-05-12'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executionDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTime = executionDate.split(\"-\")\n",
    "year = runTime[0]\n",
    "month = runTime[1]\n",
    "day = runTime[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession \\\n",
    "   .builder \\\n",
    "   .appName(\"Daily Gross Revenue Report\") \\\n",
    "   .config('hive.exec.dynamic.partition', 'true') \\\n",
    "   .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "   .config('spark.sql.warehouse.dir', 'hdfs://localhost:9000/user/hive/warehouse') \\\n",
    "   .enableHiveSupport() \\\n",
    "   .getOrCreate()\n",
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to spark df\n",
    "orders_df = spark.read.parquet('hdfs://localhost:9000/datalake/orders').drop(\"year\", \"month\", \"day\")\n",
    "order_detail_df = spark.read.parquet('hdfs://localhost:9000/datalake/order_detail').drop(\"year\", \"month\", \"day\")\n",
    "products_df = spark.read.parquet('hdfs://localhost:9000/datalake/products').drop(\"year\", \"month\", \"day\", \"created_at\")\n",
    "inventory_df = spark.read.parquet('hdfs://localhost:9000/datalake/inventory').drop(\"year\", \"month\", \"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+----------+\n",
      "| id|quantity|created_at|product_id|\n",
      "+---+--------+----------+----------+\n",
      "|  1|       1|2009-01-25|    331449|\n",
      "|  2|       1|2019-09-13|    182256|\n",
      "|  3|       2|2004-05-04|    108399|\n",
      "|  4|       3|2011-02-20|     81461|\n",
      "|  5|       3|2007-07-11|    136274|\n",
      "+---+--------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------+-------+\n",
      "| id| total|    payment|order_id|user_id|\n",
      "+---+------+-----------+--------+-------+\n",
      "|  1|710051|credit_card|       1| 209279|\n",
      "|  2|375643|       cash|       2| 242546|\n",
      "|  3|975362|       cash|       3| 135215|\n",
      "|  4|417644|credit_card|       4| 111433|\n",
      "|  5|481473|credit_card|       5|  44346|\n",
      "+---+------+-----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "order_detail_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+------------+------------+\n",
      "| id|    make|         model|    category|inventory_id|\n",
      "+---+--------+--------------+------------+------------+\n",
      "|  1|     BMW|      5 Series|Sedan, Wagon|      999830|\n",
      "|  2| Mercury| Grand Marquis|       Sedan|      988335|\n",
      "|  3|   Honda|          CR-V|         SUV|      986788|\n",
      "|  4|Cadillac|           XT5|         SUV|      986910|\n",
      "|  5|  Nissan|Titan Crew Cab|      Pickup|      988637|\n",
      "+---+--------+--------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|quantity|\n",
      "+---+--------+\n",
      "|  1|     355|\n",
      "|  2|     492|\n",
      "|  3|     269|\n",
      "|  4|     394|\n",
      "|  5|     239|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inventory_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = orders_df \\\n",
    "    .filter(orders_df[\"created_at\"] == \"2019-09-13\") \\\n",
    "    .join(order_detail_df, orders_df[\"id\"] == order_detail_df[\"order_id\"], \"inner\") \\\n",
    "    .join(products_df, orders_df[\"product_id\"] == products_df[\"id\"], \"inner\") \\\n",
    "    .join(inventory_df.select(f.col(\"quantity\").alias(\"inv_quantity\"), f.col(\"id\")), products_df[\"inventory_id\"] == inventory_df[\"id\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+----------+-------+------+-----------+--------+-------+------+---------+--------------------+--------+------------+------------+------+\n",
      "|     id|quantity|created_at|product_id|     id| total|    payment|order_id|user_id|    id|     make|               model|category|inventory_id|inv_quantity|    id|\n",
      "+-------+--------+----------+----------+-------+------+-----------+--------+-------+------+---------+--------------------+--------+------------+------------+------+\n",
      "|1288949|       1|2019-09-13|     31260|1288949|728643|credit_card| 1288949|  82492| 31260|      BMW|                X6 M|     SUV|      978802|         384|978802|\n",
      "| 331439|       3|2019-09-13|    283893| 331439|858674| instalment|  331439| 136994|283893|    Acura|                 TSX|   Sedan|      997702|         255|997702|\n",
      "| 462327|       1|2019-09-13|    196266| 462327|974749|credit_card|  462327| 269750|196266|    MAZDA|B-Series Regular Cab|  Pickup|      981583|         371|981583|\n",
      "|1601604|       1|2019-09-13|    141253|1601604|482948| instalment| 1601604| 198968|141253|Chevrolet|             Beretta|   Coupe|      990654|         109|990654|\n",
      "|1776090|       2|2019-09-13|     97582|1776090|462752|       cash| 1776090| 179206| 97582|    Buick|             LeSabre|   Sedan|      992371|         341|992371|\n",
      "+-------+--------+----------+----------+-------+------+-----------+--------+-------+------+---------+--------------------+--------+------------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pre_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df = pre_df.groupBy(\"Make\", \"Model\", \"Category\", \"product_id\", \"inv_quantity\") \\\n",
    "    .agg(\n",
    "        f.sum(\"quantity\").alias(\"Sales\"),\n",
    "        f.sum(\"total\").alias(\"Revenue\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = map_df \\\n",
    "    .withColumn(\"LetfOver\", f.col(\"inv_quantity\") - f.col(\"Sales\")) \\\n",
    "    .withColumn(\"year\", f.lit(year)) \\\n",
    "    .withColumn(\"month\", f.lit(month)) \\\n",
    "    .withColumn(\"day\", f.lit(day)) \\\n",
    "    .select(\"Make\", \"Model\", \"Category\", \"Sales\", \"Revenue\", \"year\", \"month\", \"day\", \"LetfOver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.sql(\"show databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"create database test\")\n",
    "# df1 = spark.sql(\"show databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.sql(\"describe database reports\")\n",
    "# df.select('info_value').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"create database reports\")\n",
    "# df2 = spark.sql(\"describe database reports\")\n",
    "# df2.select('info_value').collect()\n",
    "# spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 01:25:14 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:14 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 01:25:14 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:14 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 01:25:14 INFO HiveMetaStore: 0: get_database: reports\n",
      "23/08/03 01:25:14 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_database: reports\t\n",
      "23/08/03 01:25:14 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:14 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 01:25:14 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:14 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 01:25:15 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:15 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 01:25:44 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:44 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 01:25:44 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:44 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 01:25:45 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:45 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 01:25:45 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:45 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 01:25:45 INFO Hive: New loading path = hdfs://localhost:9000/user/hive/warehouse/reports.db/daily_gross_revenue/.hive-staging_hive_2023-08-03_01-25-15_104_2323326712391549837-1/-ext-10000/year=2020/month=05/day=12 with partSpec {year=2020, month=05, day=12}\n",
      "23/08/03 01:25:46 INFO HiveMetaStore: 2: get_partition_with_auth : db=reports tbl=daily_gross_revenue[2020,05,12]\n",
      "23/08/03 01:25:46 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_partition_with_auth : db=reports tbl=daily_gross_revenue[2020,05,12]\t\n",
      "23/08/03 01:25:46 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/08/03 01:25:46 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/08/03 01:25:46 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/08/03 01:25:46 INFO HiveMetaStore: 2: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "23/08/03 01:25:46 INFO ObjectStore: ObjectStore, initialize called\n",
      "23/08/03 01:25:46 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY\n",
      "23/08/03 01:25:46 INFO ObjectStore: Initialized ObjectStore\n",
      "23/08/03 01:25:46 INFO FileUtils: Creating directory if it doesn't exist: hdfs://localhost:9000/user/hive/warehouse/reports.db/daily_gross_revenue/year=2020/month=05/day=12\n",
      "23/08/03 01:25:47 INFO HiveMetaStore: 2: add_partition : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:47 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=add_partition : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 01:25:48 INFO Hive: Loaded 1 partitions\n",
      "23/08/03 01:25:48 INFO HiveMetaStore: 0: get_database: reports\n",
      "23/08/03 01:25:48 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_database: reports\t\n",
      "23/08/03 01:25:48 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:48 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 01:25:48 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 01:25:48 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n"
     ]
    }
   ],
   "source": [
    "result_df.write \\\n",
    "    .format(\"hive\") \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"reports.daily_gross_revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
