{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input** : `executionDate` -> transfrom data at this date to save to Data Warehouse <br>\n",
    "**Output**: data at `executionDate` is added in Data Warehouse with partition: year-> month->day <br>\n",
    "**Code process**:\n",
    "- Load data from HDFS Data Lake at `executionDate` with Spark to Dataframes.\n",
    "- Transfrom data from DataFrames has just create with these steps:  \n",
    "    - Join dataframes\n",
    "    - Aggreate data to get important figures\n",
    "    - Prepare results dataframe by select important columns\n",
    "- Write results dataframe to Data Warehouse at *daily_gross_revenue* table in *reports* database with partition: year->month-> day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Neccessary Libraris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkConf, SparkContext, HiveContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receive executionDate argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "executionDate = input(\"Input date you want transform data from HDFS DataLake and save to Hive Storage: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-05-13'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executionDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "runTime = executionDate.split(\"-\")\n",
    "year = runTime[0]\n",
    "month = runTime[1]\n",
    "day = runTime[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from HDFS Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "23/08/03 14:49:03 WARN Utils: Your hostname, bigdata-etl resolves to a loopback address: 127.0.1.1; using 192.168.85.128 instead (on interface ens33)\n",
      "23/08/03 14:49:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/03 14:49:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/03 14:49:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# create spark session\n",
    "spark = SparkSession \\\n",
    "   .builder \\\n",
    "   .appName(\"Daily Gross Revenue Report\") \\\n",
    "   .config('hive.exec.dynamic.partition', 'true') \\\n",
    "   .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "   .config('spark.sql.warehouse.dir', 'hdfs://localhost:9000/user/hive/warehouse') \\\n",
    "   .enableHiveSupport() \\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load data to spark df\n",
    "orders_df = spark.read.parquet('hdfs://localhost:9000/datalake/orders').drop(\"year\", \"month\", \"day\")\n",
    "order_detail_df = spark.read.parquet('hdfs://localhost:9000/datalake/order_detail').drop(\"year\", \"month\", \"day\")\n",
    "products_df = spark.read.parquet('hdfs://localhost:9000/datalake/products').drop(\"year\", \"month\", \"day\", \"created_at\")\n",
    "inventory_df = spark.read.parquet('hdfs://localhost:9000/datalake/inventory').drop(\"year\", \"month\", \"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+----------+\n",
      "| id|quantity|created_at|product_id|\n",
      "+---+--------+----------+----------+\n",
      "|  1|       1|2009-01-25|    331449|\n",
      "|  2|       1|2019-09-13|    182256|\n",
      "|  3|       2|2004-05-04|    108399|\n",
      "|  4|       3|2011-02-20|     81461|\n",
      "|  5|       3|2007-07-11|    136274|\n",
      "+---+--------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+--------+-------+\n",
      "| id| total|    payment|order_id|user_id|\n",
      "+---+------+-----------+--------+-------+\n",
      "|  1|710051|credit_card|       1| 209279|\n",
      "|  2|375643|       cash|       2| 242546|\n",
      "|  3|975362|       cash|       3| 135215|\n",
      "|  4|417644|credit_card|       4| 111433|\n",
      "|  5|481473|credit_card|       5|  44346|\n",
      "+---+------+-----------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "order_detail_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+------------+------------+\n",
      "| id|    make|         model|    category|inventory_id|\n",
      "+---+--------+--------------+------------+------------+\n",
      "|  1|     BMW|      5 Series|Sedan, Wagon|      999830|\n",
      "|  2| Mercury| Grand Marquis|       Sedan|      988335|\n",
      "|  3|   Honda|          CR-V|         SUV|      986788|\n",
      "|  4|Cadillac|           XT5|         SUV|      986910|\n",
      "|  5|  Nissan|Titan Crew Cab|      Pickup|      988637|\n",
      "+---+--------+--------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|quantity|\n",
      "+---+--------+\n",
      "|  1|     355|\n",
      "|  2|     492|\n",
      "|  3|     269|\n",
      "|  4|     394|\n",
      "|  5|     239|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inventory_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggreate data to get important figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = orders_df \\\n",
    "    .filter(orders_df[\"created_at\"] == executionDate) \\\n",
    "    .join(order_detail_df, orders_df[\"id\"] == order_detail_df[\"order_id\"], \"inner\") \\\n",
    "    .join(products_df, orders_df[\"product_id\"] == products_df[\"id\"], \"inner\") \\\n",
    "    .join(inventory_df.select(f.col(\"quantity\").alias(\"inv_quantity\"), f.col(\"id\")), products_df[\"inventory_id\"] == inventory_df[\"id\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df = pre_df.groupBy(\"Make\", \"Model\", \"Category\", \"product_id\", \"inv_quantity\") \\\n",
    "    .agg(\n",
    "        f.sum(\"quantity\").alias(\"Sales\"),\n",
    "        f.sum(\"total\").alias(\"Revenue\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = map_df \\\n",
    "    .withColumn(\"LetfOver\", f.col(\"inv_quantity\") - f.col(\"Sales\")) \\\n",
    "    .withColumn(\"year\", f.lit(year)) \\\n",
    "    .withColumn(\"month\", f.lit(month)) \\\n",
    "    .withColumn(\"day\", f.lit(day)) \\\n",
    "    .select(\"Make\", \"Model\", \"Category\", \"Sales\", \"Revenue\", \"year\", \"month\", \"day\", \"LetfOver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results dataframe to Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/03 14:50:05 INFO HiveConf: Found configuration file file:/usr/local/spark/conf/hive-site.xml\n",
      "23/08/03 14:50:06 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/08/03 14:50:06 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/08/03 14:50:06 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "23/08/03 14:50:06 INFO ObjectStore: ObjectStore, initialize called\n",
      "23/08/03 14:50:07 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n",
      "23/08/03 14:50:07 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored\n",
      "23/08/03 14:50:14 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n",
      "23/08/03 14:50:20 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY\n",
      "23/08/03 14:50:20 INFO ObjectStore: Initialized ObjectStore\n",
      "23/08/03 14:50:21 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "23/08/03 14:50:21 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hadoop@127.0.1.1\n",
      "23/08/03 14:50:21 INFO HiveMetaStore: Added admin role in metastore\n",
      "23/08/03 14:50:21 INFO HiveMetaStore: Added public role in metastore\n",
      "23/08/03 14:50:21 INFO HiveMetaStore: No user is added in admin role, since config is empty\n",
      "23/08/03 14:50:21 INFO HiveMetaStore: 0: get_database: default\n",
      "23/08/03 14:50:21 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_database: default\t\n",
      "23/08/03 14:50:21 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:21 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:22 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:22 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:22 INFO HiveMetaStore: 0: get_database: reports\n",
      "23/08/03 14:50:22 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_database: reports\t\n",
      "23/08/03 14:50:22 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:22 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:22 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:22 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:23 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:23 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:52 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:52 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:52 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:52 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:53 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:53 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:53 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:53 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:53 INFO Hive: New loading path = hdfs://localhost:9000/user/hive/warehouse/reports.db/daily_gross_revenue/.hive-staging_hive_2023-08-03_14-50-23_321_6542270454194797700-1/-ext-10000/year=2018/month=05/day=13 with partSpec {year=2018, month=05, day=13}\n",
      "23/08/03 14:50:53 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=f6f68f56-e7c6-42b2-b755-4358227061e4, clientType=HIVECLI]\n",
      "23/08/03 14:50:53 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/08/03 14:50:53 INFO HiveMetaStore: 1: get_partition_with_auth : db=reports tbl=daily_gross_revenue[2018,05,13]\n",
      "23/08/03 14:50:53 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_partition_with_auth : db=reports tbl=daily_gross_revenue[2018,05,13]\t\n",
      "23/08/03 14:50:53 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/08/03 14:50:53 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/08/03 14:50:53 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "23/08/03 14:50:53 INFO ObjectStore: ObjectStore, initialize called\n",
      "23/08/03 14:50:53 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY\n",
      "23/08/03 14:50:53 INFO ObjectStore: Initialized ObjectStore\n",
      "23/08/03 14:50:54 INFO FileUtils: Creating directory if it doesn't exist: hdfs://localhost:9000/user/hive/warehouse/reports.db/daily_gross_revenue/year=2018/month=05/day=13\n",
      "23/08/03 14:50:56 INFO HiveMetaStore: 1: add_partition : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:56 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=add_partition : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:57 INFO Hive: Loaded 1 partitions\n",
      "23/08/03 14:50:58 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "23/08/03 14:50:58 INFO HiveMetaStore: 0: Cleaning up thread local RawStore...\n",
      "23/08/03 14:50:58 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=Cleaning up thread local RawStore...\t\n",
      "23/08/03 14:50:58 INFO HiveMetaStore: 0: Done cleaning up thread local RawStore\n",
      "23/08/03 14:50:58 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=Done cleaning up thread local RawStore\t\n",
      "23/08/03 14:50:58 INFO HiveMetaStore: 0: get_database: reports\n",
      "23/08/03 14:50:58 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_database: reports\t\n",
      "23/08/03 14:50:58 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "23/08/03 14:50:58 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "23/08/03 14:50:58 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "23/08/03 14:50:58 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "23/08/03 14:50:58 INFO ObjectStore: ObjectStore, initialize called\n",
      "23/08/03 14:50:58 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY\n",
      "23/08/03 14:50:58 INFO ObjectStore: Initialized ObjectStore\n",
      "23/08/03 14:50:58 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:58 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:58 INFO HiveMetaStore: 0: get_table : db=reports tbl=daily_gross_revenue\n",
      "23/08/03 14:50:58 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_table : db=reports tbl=daily_gross_revenue\t\n",
      "23/08/03 14:50:58 INFO HiveMetaStore: 0: get_database: global_temp\n",
      "23/08/03 14:50:58 INFO audit: ugi=hadoop\tip=unknown-ip-addr\tcmd=get_database: global_temp\t\n",
      "23/08/03 14:50:58 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    }
   ],
   "source": [
    "result_df.write \\\n",
    "    .format(\"hive\") \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"reports.daily_gross_revenue\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
